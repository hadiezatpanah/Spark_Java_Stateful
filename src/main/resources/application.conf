
application {
    SparkSession {
        spark {
            master = yarn
            app.name = SessionManagement
            driver.memory=16g
            executor.memory=8g
            executor.instances=4
            executor.cores=2
            cleaner.referenceTracking.cleanCheckpoints=true
            cleaner.referenceTracking.blocking=true
            cleaner.referenceTracking.blocking.shuffle=true
            sql.sources.partitionOverwriteMode=dynamic
            sql.shuffle.partitions=256
            streaming.concurrentJobs=4
            sql.streaming.metricsEnabled=true
            hadoop.fs.hdfs.impl.disable.cache=true
            scheduler.mode=FAIR
;             sql.legacy.allowCreatingManagedTableUsingNonemptyLocation=true
            sql.broadcastTimeout=3600
            driver.maxResultSize=0
            streaming.stopGracefullyOnShutdown=true
;             sql.session.timeZone=Asia/tehran
        }
        hive.exec.dynamic.partition=true
        hive.exec.dynamic.partition.mode=nonstrict
    }
    Transform {
        state.timeout = 1
    }
}


prod {

}

test {
    Extract {
            sessions {
                format = kafka
                kafka.bootstrap.servers = "localhost:9092"
                subscribe = session_topic
                startingOffsets = earliest
                failOnDataLoss = false
                maxOffsetsPerTrigger = 1000
                schema = {"type":"struct","fields":[{"name":"userID","type":"string","nullable":true,"metadata":{}},{"name":"sessionID","type":"string","nullable":true,"metadata":{}},{"name":"eventType","type":"string","nullable":true,"metadata":{}},{"name":"timestamp","type":"timestamp","nullable":true,"metadata":{}}]}

            }
        }

    Transform {

    }

    Load {
        output {
            options {
                url = "jdbc:oracle:thin:@//localhost:1521/test"
                dbtable = joined_session
                numPartitions = 10
                batchsize = 50000
                user = test
                password = test
                driver = "oracle.jdbc.driver.OracleDriver"
            }
            format = jdbc
            mode = append
            path = ""
        }
        options {
            checkpointLocation = src/main/resources/CP/
            trigger.processingTime = "2 minute"
            output.mode = append
        }
    }
}


dev {
    SparkSession {
        spark {
            master = "local[*]"
        }
    }
    Extract {
        sessions {
            format = MemoryStream
        }
    }
    Load {
        output {
            options {
                header = true
                delimiter = ","
            }
            format = csv
            path = src/main/resources/output
            mode = append
        }
        options {
            checkpointLocation = src/main/resources/CP/
            trigger.processingTime = "1 minute"
            output.mode = append
        }
    }
}

